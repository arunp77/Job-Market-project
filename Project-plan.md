# Various steps to work on the project

## Step 0 
### Framing (first meeting):
- Introduction of each team member
- Explanation of the project framework (the different stages)

## Step 1 
### Discovery of available data sources & Data organization: Deadline 10 February
- Define the context and scope of the project (don't underestimate this step)
- Get to grips with the different data sources (explore the APIs provided but available to you, the web pages for which you will apply webs-scraping)
- You will be asked to organize the data via different databases:
    - Relational
    - NoSQL
- You will have to think about the data architecture, including how to link the different data together.
- Deliverable :
    - Report explaining the different data sources with examples of collected data
    - Any document explaining the chosen architecture (UML diagram)
    - File implementing the databases
    - Query file

## Step 2 
### Data consumption : Deadline Feb 17th 2024
- Once your data is organized, it needs to be consumed, this is not the initial role of a Data Engineer, but for the data pipeline to be complete, you need to have this part.
- It will be expected to make a notebook where you do Machine Learning on it or a dashboard with Dash

## Step 3
### Deployment: Deadline  Feb 24th 2024
- Create an API of the Machine Learning model or Dash application
- Perform unit tests on your API
- Contain this API via Docker and databases

## Step 4 
### Automate the flows: Optional step (Mar 10th 2024)
- Automate the various previous steps so that the application is continuously functional
- Set up a CI/CD pipeline to efficiently update your application

## Step 5
### Demonstration of the application + Support (30 minutes): Mar 18th 2024
- Explain the progress of your project
- Explain the architecture chosen when organizing the data
- Show that the application is functional
